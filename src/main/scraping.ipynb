{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4fe1d77-bedb-4109-a2c3-c1c016b469c4",
   "metadata": {},
   "source": [
    "\n",
    "- imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65c2fd6-b47b-43bf-a2e5-d6fb114ec258",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# from main.utils import get_logger\n",
    "# from main.scraper import (fetch_response, make_soup, extract_blocks, pager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19dd0de-440b-4afd-b113-1d492373106d",
   "metadata": {},
   "source": [
    "\n",
    "- modules ship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f06567-e710-4590-84d9-fe3e9d39ea6d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from urllib3.util.retry import Retry\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def fetch_response(url, session=None, timeout=10):\n",
    "    if not isinstance(url, str) or not url.startswith(\"http\"):\n",
    "        logger.warning(f\"Invalid URL: {url}\")\n",
    "        return None\n",
    "    session = session or requests.Session()\n",
    "\n",
    "    if not hasattr(session, \"_retry_configured\"):\n",
    "        retry_strategy = Retry(\n",
    "            total=3,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            allowed_methods=[\"GET\", \"HEAD\"],\n",
    "            backoff_factor=1,\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "        session.mount(\"http://\", adapter)\n",
    "        session.mount(\"https://\", adapter)\n",
    "        session._retry_configured = True\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\",\n",
    "        'Accept-Charset': 'utf-8',\n",
    "    }\n",
    "    try:\n",
    "        response = session.get(url, headers=headers, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "        response.encoding = 'utf-8'\n",
    "        logger.info(f\"fetched: {url} [{response.status_code}]\")\n",
    "        return response\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        logger.warning(f\"HTTP error occurred:\\n{errh}\")\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        logger.warning(f\"{type(err).__name__}\\n{err}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Unhandled exception:\\n{type(e).__name__}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_soup(response=None, url=None):\n",
    "    if isinstance(url, str):\n",
    "        response = fetch_response(url)\n",
    "    if not (response and response.status_code == 200):\n",
    "        logger.warning(f\"Couldn't make soup with: {response}\")\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    return soup\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def safe_soup_select(soup, selector):\n",
    "    try:\n",
    "        return soup.select(selector)\n",
    "    except AttributeError as err:\n",
    "        logger.error(f\"{selector} {soup} returning {None}: {err}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Invalid CSS selector '{selector}', returning None: {e}\")\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "def extract_blocks(soup, block_selector, fields):\n",
    "    \"\"\"\n",
    "    Extract structured data blocks from soup.\n",
    "\n",
    "    :param soup: BeautifulSoup object\n",
    "    :param block_selector: CSS selector to identify each block\n",
    "    :param fields: dict of {field_name: (selector, attr, default)}\n",
    "                   - attr defaults to \"text\"\n",
    "                   - default defaults to None\n",
    "    :return: List of dicts with extracted data\n",
    "    \"\"\"\n",
    "    blocks = safe_soup_select(soup, block_selector)\n",
    "    if not blocks:\n",
    "        logger.info(f\"No blocks found with selector '{block_selector}'.\")\n",
    "        return []\n",
    "\n",
    "    extracted_data = []\n",
    "    for i, block in enumerate(blocks, start=1):\n",
    "        block_data = {}\n",
    "        for field_name, field_def in fields.items():\n",
    "            try:\n",
    "                selector, *rest = field_def\n",
    "                attr = rest[0] if len(rest) > 0 else \"text\"\n",
    "                field_default = rest[1] if len(rest) > 1 else None\n",
    "                attr = attr or \"text\"\n",
    "                \n",
    "                elements = safe_soup_select(block, selector)\n",
    "                if not elements:\n",
    "                    logger.info(f\"Invalid selector '{selector}' for field '{field_name}' or No such elements found block '{block}'\")\n",
    "                    elements = []\n",
    "\n",
    "                values = []\n",
    "                for el in elements:\n",
    "                    try:\n",
    "                        if attr == \"text\":\n",
    "                            value = el.get_text(strip=True, separator=\" \")\n",
    "                        elif attr == \"html\":\n",
    "                            value = str(el)\n",
    "                        elif attr == \"inner_html\":\n",
    "                            value = \"\".join(str(c) for c in el.contents)\n",
    "                        else:\n",
    "                            value = el.get(attr)\n",
    "\n",
    "                        if value:\n",
    "                            values.append(value)\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error extracting attr '{attr}' for field '{field_name}': {e}\")\n",
    "\n",
    "                block_data[field_name] = values if values else field_default\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Unexpected error in field '{field_name}' (block {i}): {e}\")\n",
    "                block_data[field_name] = None\n",
    "\n",
    "        extracted_data.append(block_data)\n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_multiple_blocks(soup, extraction_jobs):\n",
    "    \"\"\"\n",
    "    Extracts multiple sets of structured data blocks from a single soup object.\n",
    "\n",
    "    :param soup: BeautifulSoup object\n",
    "    :param extraction_jobs: A dictionary mapping data type names to a dict\n",
    "                            containing 'block_selector' and 'fields'.\n",
    "    :return: A dictionary of lists, where each key is a data type (e.g., 'books', 'categories')\n",
    "             and the value is a list of extracted data dictionaries.\n",
    "    \"\"\"\n",
    "    if not soup:\n",
    "        logger.error(\"extract_multiple_blocks() called with no soup object.\")\n",
    "        return {}\n",
    "\n",
    "    all_extracted_data = {}\n",
    "    for job_name, job_config in extraction_jobs.items():\n",
    "        block_selector = job_config.get(\"block_selector\")\n",
    "        fields = job_config.get(\"fields\")\n",
    "\n",
    "        # Reuse the existing extract_blocks function for each job\n",
    "        extracted_data = extract_blocks(soup, block_selector, fields)\n",
    "        all_extracted_data[job_name] = extracted_data\n",
    "    \n",
    "    return all_extracted_data\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def pager(base, max_pages=3, fetcher=make_soup, callback=extract_multiple_blocks, **kwargs):\n",
    "    \"\"\"\n",
    "    Iterate through paginated links starting from `base`.\n",
    "    :param callback: A function that can process a single soup object and return\n",
    "                     a dictionary of multiple data sets.\n",
    "    \"\"\"\n",
    "    pages = {}\n",
    "    counter = 1\n",
    "    current_url = base\n",
    "\n",
    "    while counter <= max_pages:\n",
    "        page_soup = fetcher(url=current_url)\n",
    "        if not page_soup:\n",
    "            break\n",
    "\n",
    "        extracted_data = callback(page_soup, **kwargs)\n",
    "        pages[current_url] = extracted_data\n",
    "\n",
    "        next_hrefs = pages[current_url][\"links\"][0].get('next_href')\n",
    "        if not next_hrefs:\n",
    "            break\n",
    "            \n",
    "        current_url = urljoin(current_url, next_hrefs[0])\n",
    "        counter += 1\n",
    "\n",
    "    return pages\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "transformer = lambda data: pd.DataFrame([\n",
    "    {key: val[0] if isinstance(val, list) and val else None for key, val in record.items()}\n",
    "    for record in data\n",
    "]) if data else pd.DataFrame(data) # get 0th index of all list type values make it new key value\n",
    "\n",
    "\n",
    "\n",
    "def join_page_records(scraped_data, job_name):\n",
    "    all_data = []\n",
    "    for page_url in scraped_data:\n",
    "        if job_name in scraped_data[page_url] and scraped_data[page_url][job_name]:\n",
    "            all_data.extend(scraped_data[page_url][job_name])\n",
    "    return all_data\n",
    "\n",
    "# --------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import shutil\n",
    "\n",
    "_logger = None\n",
    "def get_logger(name=\"books_scraper\"):\n",
    "    \"\"\"Singleton logger to be used across the project.\n",
    "    \n",
    "    This version explicitly deletes the logs directory and its contents\n",
    "    before recreating it to ensure a clean start for each program run.\n",
    "    \"\"\"\n",
    "    global _logger\n",
    "    if _logger:\n",
    "        return _logger\n",
    "\n",
    "    if os.path.exists(\"logs\"):\n",
    "        shutil.rmtree(\"logs\")\n",
    "    \n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "    \n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    if not logger.handlers:\n",
    "        formatter = logging.Formatter(\n",
    "            \"[%(levelname)s] %(filename)s:%(lineno)d %(funcName)s() %(message)s\"\n",
    "        )\n",
    "\n",
    "        file_handler = logging.FileHandler(\"logs/scraping.log\", mode=\"w\")\n",
    "        file_handler.setFormatter(formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "        console = logging.StreamHandler()\n",
    "        console.setFormatter(formatter)\n",
    "        logger.addHandler(console)\n",
    "\n",
    "    _logger = logger\n",
    "    return logger\n",
    "\n",
    "logger = get_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca26afb4-1c94-433b-b0c4-9b77ca016465",
   "metadata": {},
   "source": [
    "\n",
    "- extraction fields / miscellaneous(extras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadd1ed8-4166-43b8-b77c-e3261c0d9dfe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "allinone_extractions = {\n",
    "    \"products\": {\n",
    "        \"block_selector\": \".product-wrapper.card-body\",\n",
    "        \"fields\": {\n",
    "            \"img_alt\": (\"img\", \"alt\"),\n",
    "            \"img_src\": (\"img\", \"src\"),\n",
    "            \"price\": (\".caption span\", \"text\"),\n",
    "            \"currency\": (\".caption meta\", \"content\"),\n",
    "            \"title\": (\".caption h4 a\", \"title\"),\n",
    "            \"description\": (\".caption p.description\", \"text\"),\n",
    "            \"review-count\": (\".ratings p.review-count span\", \"text\"),\n",
    "            \"rating\": (\".ratings p\", \"data-rating\"),\n",
    "        }\n",
    "    },\n",
    "    \"links\": {\n",
    "        \"block_selector\": \".pagination\",\n",
    "        \"fields\": {\n",
    "            \"next_href\": (\"a.next\", \"href\"),\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scrapingcourse_extractions = {\n",
    "    \"products\": {\n",
    "        \"block_selector\": \".product-item\",\n",
    "        \"fields\": {\n",
    "            \"name\": (\".product-name\", \"text\"),\n",
    "            \n",
    "            \"price\": (\".product-price\", \"text\"),\n",
    "            \n",
    "            \"image-alt\": (\".product-image\", \"alt\"),\n",
    "            # \"image-text\": (\".product-image\", \"text\"),            \n",
    "            # \"image-sizes\": (\".product-image\", \"sizes\"),\n",
    "            # \"image-hieght\": (\".product-image\", \"hieght\"),\n",
    "            \"image-width\": (\".product-image\", \"width\"),\n",
    "            # \"image-srcset\": (\".product-image\", \"srcset\"),\n",
    "            \n",
    "            # \"id\": (\".button\", \"data-product_id\"),\n",
    "            # \"sku\": (\".button\", \"data-product_sku\"),\n",
    "            # \"quantity\": (\".button\", \"data-quantity\"),\n",
    "            # \"aria-describedby\": (\".button\", \"aria-describedby\"),\n",
    "        }\n",
    "    },\n",
    "    \"links\": {\n",
    "        \"block_selector\": \"#pagination-container\",\n",
    "        \"fields\": {\n",
    "            \"next_href\": (\".next-page\", \"href\"),\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "all_book_extractions = {\n",
    "    \"books\": {\n",
    "        \"block_selector\": \".product_pod\",\n",
    "        \"fields\": {\n",
    "            \"img_href\": (\".image_container a\", \"href\"),\n",
    "            \"img_alt\": (\".image_container a img\", \"alt\"),\n",
    "            \"img_src\": (\".image_container a img\", \"src\"),\n",
    "            \"rating\": (\".star-rating\", \"class\"),\n",
    "            \"book_href\": (\"h3 a\", \"href\"),\n",
    "            \"book_text\": (\"h3 a\", \"text\"),\n",
    "            \"price\": (\".price_color\", \"text\"),\n",
    "            \"availability\": (\".availability\", \"text\"),\n",
    "        }\n",
    "    },\n",
    "    \"categories\": {\n",
    "        \"block_selector\": \".nav-list ul\",\n",
    "        \"fields\": {\n",
    "            \"category_href\": (\"a\", \"href\"),\n",
    "            \"category_text\": (\"a\", \"text\"),\n",
    "        }\n",
    "    },\n",
    "    \"links\": {\n",
    "        \"block_selector\": \".pager .next\",\n",
    "        \"fields\": {\n",
    "            \"next_href\": (\"a\", \"href\"),\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "output_dir = Path(\"src/main/data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# lambda (selector, attr): (_.get(attr) if (_:= book.select_one(selector)) else \"\") # to avoid NoneType has no attr x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1a33dd-9ca2-4d51-a0cb-4e9eaed3c23a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf025db9-c633-4fcd-8fb9-94d3008c4d65",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### all-in-one / books-to-scrape\n",
    "-\n",
    "- - requests\n",
    "- - httpx\n",
    "\n",
    "-\n",
    "- - bs4.beutifulSoup  .select_one(\"css\")\n",
    "- - selectorlax.parser HTMLParser   .css_first(\"css\")\n",
    "\n",
    "-\n",
    "- - pagination - soup fetch\n",
    "- - common pager component types\n",
    "- - - with next page button\n",
    "    - - without next button\n",
    "    - - - url-based (https://example.com/products?page=)\n",
    "    - - - infinit scroll\n",
    "    - - - load more button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc164569-2009-45ac-a047-1928d0a3faad",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# scraped_data = pager(\n",
    "#     base=\"https://webscraper.io/test-sites/e-commerce/static/computers/laptops\",\n",
    "#     extraction_jobs=allinone_extractions\n",
    "# ) or {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6f10fc-541c-4c6c-a646-eb9b3bcac53b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# scraped_data_1_phones = pager(\n",
    "#     base=\"https://webscraper.io/test-sites/e-commerce/static/phones/touch\",\n",
    "#     extraction_jobs=allinone_extractions\n",
    "# ) or {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a70c92-0b3e-4a25-99fe-db91a0ca3526",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f437c2b1-a7d5-4d5b-bce9-043a07d87fb0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "scraped_data_2 = pager(\n",
    "    base=\"https://books.toscrape.com\",\n",
    "    extraction_jobs=all_book_extractions\n",
    ") or {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c061f9a9-d54b-4ec2-9d07-4af2acecf973",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfd3ff3-17dc-4998-ba72-890c4dea4b39",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# scraped_data_cloths = pager(\n",
    "#     base=\"https://www.scrapingcourse.com/pagination\",\n",
    "#     extraction_jobs=scrapingcourse_extractions\n",
    "# ) or {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187d3c89-ff5b-4bdb-9252-beb19ccc2126",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91dcd3c-6004-466a-9418-caf1c7a2f905",
   "metadata": {},
   "source": [
    "\n",
    "- preprocess / orginise data\n",
    "- - join page data\n",
    "  - make data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4407b6e-eae3-4993-b49f-9f2130f5129a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# _ =join_page_records(scraped_data, 'products')\n",
    "# df = transformer(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7517d25a-e55a-4a7b-a88d-c67d88655f9e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# _ =join_page_records(scraped_data_1_phones, 'products')\n",
    "# df_phones = transformer(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542a12c8-8e61-4936-a08f-c3002b5d1cf2",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bf8798-7f76-43e2-ad29-1965e15465ea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "_ = join_page_records(scraped_data_2, 'books')\n",
    "df_2 = transformer(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f988647c-75ab-450a-b474-32614ae97428",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# _ = scraped_data_2.get('https://books.toscrape.com', {}).get('categories', {})\n",
    "# categories_df = pd.DataFrame(_[0] if _ else {})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb72d90e-89ba-46eb-a379-d8780dbd42e0",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fe9057-a45f-4b47-afe5-56076dca0675",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# _ =join_page_records(scraped_data_cloths, 'products')\n",
    "# df_cloths = transformer(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a6e985-1f6b-4769-ae80-c3c569fd085b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81135e10-2520-4bb9-bbd3-50a85e79dddc",
   "metadata": {},
   "source": [
    "\n",
    "- transform\n",
    "- preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8349744f-f078-4516-8d14-dc7683571844",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_phones.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0d39e4-5023-432c-afce-d65ebba27d11",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df[\"price\"] = df[\"price\"].replace(r\"[^\\d\\.]\", \"\", regex=True).astype(float)\n",
    "# df.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0a07f-12d5-429d-b814-bd1d7044f167",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d665fbe9-95a4-4f86-88a7-4f044260f533",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_2[\"currency\"] = df_2[\"price\"].str.extract(r\"(^\\D)\")\n",
    "df_2[\"price\"] = df_2[\"price\"].replace(r\"[^\\d\\.]\", \"\", regex=True).astype(float)\n",
    "\n",
    "ratings = {\"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4, \"five\": 5}\n",
    "\n",
    "df_2[\"rating\"] = df_2[\"rating\"].apply(\n",
    "    lambda tags: ratings[_[0]] if ( _ := list(filter(lambda tag: tag in ratings, map(str.lower, tags))) ) else None\n",
    ")\n",
    "\n",
    "df_2.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3645fe-58cb-4950-8d21-5ab7e82c9535",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# categories_df.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be4ae49-98c0-407c-a695-bf5e9636cadd",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba80c7-a81e-4868-98a9-6e5ce16e2d2e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_cloths[\"currency\"] = df_cloths[\"price\"].str.extract(r\"(^\\D)\")\n",
    "# df_cloths[\"price\"] = df_cloths[\"price\"].replace(r\"[^\\d\\.]\", \"\", regex=True).astype(float)\n",
    "\n",
    "# df_cloths.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023287f5-5bea-4c2d-9f49-98e4a98d3b2c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1cac45-fa20-4c9d-89c4-a20abf173f82",
   "metadata": {},
   "source": [
    "\n",
    "- save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f897a1-8ccb-4f0d-a545-534771a4b698",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_phones.to_csv(output_dir/\"phones_allinone.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a297e5-1f97-43d0-8784-44e5ad5e066f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df.to_csv(output_dir/\"laptops_allinone.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8368b761-3bea-4a8c-9aa9-9477387308cf",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadd3270-1818-4646-b1c6-a5cccf062031",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_2.to_csv(output_dir/\"books_bookstoscrape.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c2a15f-6797-4bb2-b08b-608cc6aa91c2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# categories_df.to_csv(output_dir/\"categories_bookstoscrape.csv\", sep=\";\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1318b173-15a1-48f5-901b-18edd2142e3c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28813e7-38c1-4646-9bb6-ab60d1edec90",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_cloths.to_csv(output_dir/\"cloths_scrapingcourse.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cdffcc-6f12-465b-bf23-82fb406b67ef",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f014248-af9c-4d6b-8a3d-edfe068e4726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_item(df, url_column, base_url, block_selector, fields,max_items=5):\n",
    "    \"\"\"\n",
    "    Scrapes detailed information from a list of URLs and returns a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the URLs.\n",
    "        url_column (str): The name of the column with the relative URLs.\n",
    "        base_url (str): The base URL to join with the relative URLs.\n",
    "        block_selector (str): CSS selector for the data block on the detail page.\n",
    "        fields (dict): Dictionary of fields and their selectors.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the scraped detailed item data.\n",
    "    \"\"\"\n",
    "    all_item_data = []\n",
    "    counter = 0\n",
    "\n",
    "    for href in df[url_column]:\n",
    "        if counter >= max_items:\n",
    "            break\n",
    "        link = urljoin(base_url, \"../\"+href)\n",
    "        logger.info(f\"Scraping detail page: {link}\")\n",
    "        \n",
    "        item_soup = make_soup(url=link)\n",
    "        if not item_soup:\n",
    "            continue  # Skip if the page couldn't be fetched or parsed\n",
    "\n",
    "        # The result of extract_blocks is a list of dictionaries\n",
    "        scraped_records = extract_blocks(item_soup, block_selector, fields)\n",
    "        \n",
    "        if scraped_records:\n",
    "            # Add the original URL to the scraped data for merging later\n",
    "            scraped_records[0]['source_url'] = link\n",
    "            all_item_data.extend(scraped_records)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    item_df = transformer(all_item_data)\n",
    "    \n",
    "    return item_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dd8a81-33c8-4f69-81cc-65f368eb85f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_detail_extractions = {\n",
    "    'block_selector': '.product_page',\n",
    "    'fields': {\n",
    "        'product_description': ('#product_description ~ p', 'text'),\n",
    "        'upc': ('.table.table-striped tr:nth-of-type(1) td', 'text'),\n",
    "        'product_type': ('.table.table-striped tr:nth-of-type(2) td', 'text'),\n",
    "        'price_excl_tax': ('.table.table-striped tr:nth-of-type(3) td', 'text'),\n",
    "        'price_incl_tax': ('.table.table-striped tr:nth-of-type(4) td', 'text'),\n",
    "        'tax': ('.table.table-striped tr:nth-of-type(5) td', 'text'),\n",
    "        'availability': ('.table.table-striped tr:nth-of-type(6) td', 'text'),\n",
    "        'number_of_reviews': ('.table.table-striped tr:nth-of-type(7) td', 'text'),\n",
    "        'image_url': ('#product_gallery .thumbnail img', 'src'),\n",
    "        'book_text': ('.product_main h1', 'text')\n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "# Scrape the detailed information for each book\n",
    "detailed_books_df = scrape_item(\n",
    "    df=df_2,\n",
    "    url_column='book_href',  # This is the column with the relative URLs\n",
    "    base_url='https://books.toscrape.com/catalogue/', # base url needed for this site.\n",
    "    block_selector=book_detail_extractions['block_selector'],\n",
    "    fields=book_detail_extractions['fields']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace90460-6f88-4467-b4d7-d4525b9dd2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df_2, detailed_books_df, on=\"book_text\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
