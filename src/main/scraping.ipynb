{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6c280a6-6425-4684-a587-a2f1d94a211a",
   "metadata": {},
   "source": [
    "--- imports ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65c2fd6-b47b-43bf-a2e5-d6fb114ec258",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19dd0de-440b-4afd-b113-1d492373106d",
   "metadata": {},
   "source": [
    "--- modulor definitions ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8415299f-8dbc-47c2-9ad1-32b9c172fa63",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from main.utils import get_logger\n",
    "# from main.scraper import (fetch_response, make_soup, extract_blocks, pager)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import shutil\n",
    "\n",
    "\n",
    "def fetch_response(url, session=None, timeout=10):\n",
    "    if not isinstance(url, str) or not url.startswith(\"http\"):\n",
    "        logger.warning(f\"Invalid URL: {url}\")\n",
    "        return None\n",
    "    session = session or requests.Session()\n",
    "\n",
    "    if not hasattr(session, \"_retry_configured\"):\n",
    "        retry_strategy = Retry(\n",
    "            total=3,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            allowed_methods=[\"GET\", \"HEAD\"],\n",
    "            backoff_factor=1,\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "        session.mount(\"http://\", adapter)\n",
    "        session.mount(\"https://\", adapter)\n",
    "        session._retry_configured = True\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\",\n",
    "        'Accept-Charset': 'utf-8',\n",
    "    }\n",
    "    try:\n",
    "        response = session.get(url, headers=headers, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "        logger.info(f\"fetched: {url} [{response.status_code}]\")\n",
    "        return response\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        logger.warning(f\"HTTP error occurred:\\n{errh}\")\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        logger.warning(f\"{type(err).__name__}\\n{err}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Unhandled exception:\\n{type(e).__name__}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_soup(response=None, url=None):\n",
    "    if isinstance(url, str):\n",
    "        response = fetch_response(url)\n",
    "    if not (response and response.status_code == 200):\n",
    "        logger.warning(f\"Couldn't make soup with: {response}\")\n",
    "        return None\n",
    "    \n",
    "    response.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    return soup\n",
    "\n",
    "\n",
    "\n",
    "def extract_blocks(soup, block_selector, fields):\n",
    "    \"\"\"\n",
    "    Extract structured data blocks from soup.\n",
    "\n",
    "    :param soup: BeautifulSoup object\n",
    "    :param block_selector: CSS selector to identify each block\n",
    "    :param fields: dict of {field_name: (selector, attr, default)}\n",
    "                   - attr defaults to \"text\"\n",
    "                   - default defaults to None\n",
    "    :return: List of dicts with extracted data\n",
    "    \"\"\"\n",
    "    if not soup:\n",
    "        logger.error(\"extract_blocks() called with no soup object.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        blocks = soup.select(block_selector)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Invalid block selector '{block_selector}': {e}\")\n",
    "        return []\n",
    "\n",
    "    if not blocks:\n",
    "        logger.info(f\"No blocks found with selector '{block_selector}'.\")\n",
    "        return []\n",
    "\n",
    "    extracted_data = []\n",
    "    for i, block in enumerate(blocks, start=1):\n",
    "        block_data = {}\n",
    "        for field_name, field_def in fields.items():\n",
    "            try:\n",
    "                # Unpack tuple with defaults\n",
    "                selector, attr, field_default = (list(field_def) + [\"text\", None])[0:3]\n",
    "                attr = attr or \"text\"\n",
    "\n",
    "                try:\n",
    "                    elements = block.select(selector)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Invalid selector '{selector}' for field '{field_name}': {e}\")\n",
    "                    elements = []\n",
    "\n",
    "                values = []\n",
    "                for el in elements:\n",
    "                    try:\n",
    "                        if attr == \"text\":\n",
    "                            value = el.get_text(strip=True, separator=\" \")\n",
    "                        elif attr == \"html\":\n",
    "                            value = str(el)\n",
    "                        elif attr == \"inner_html\":\n",
    "                            value = \"\".join(str(c) for c in el.contents)\n",
    "                        else:\n",
    "                            value = el.get(attr)\n",
    "\n",
    "                        if value:\n",
    "                            values.append(value)\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error extracting attr '{attr}' for field '{field_name}': {e}\")\n",
    "\n",
    "                block_data[field_name] = values if values else field_default\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Unexpected error in field '{field_name}' (block {i}): {e}\")\n",
    "                block_data[field_name] = None\n",
    "\n",
    "        extracted_data.append(block_data)\n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pager(base, max_pages, next_page_selectors, fetcher, callback, **kwargs):\n",
    "    \"\"\"\n",
    "    Iterate through paginated links starting from `base`.\n",
    "    :param callback: A function that can process a single soup object and return\n",
    "                     a dictionary of multiple data sets.\n",
    "    \"\"\"\n",
    "    pages = {}\n",
    "    counter = 1\n",
    "    current_url = base\n",
    "\n",
    "    while counter <= max_pages:\n",
    "        page_soup = fetcher(url=current_url)\n",
    "        if not page_soup:\n",
    "            break\n",
    "\n",
    "        # The callback now returns a dict of all the data from the page\n",
    "        extracted_data = callback(page_soup, **kwargs)\n",
    "        pages[current_url] = extracted_data\n",
    "\n",
    "        next_url = None\n",
    "        for block_sel, elem_sel in next_page_selectors:\n",
    "            try:\n",
    "                link_data = extract_blocks(page_soup, block_sel, {elem_sel: (elem_sel, \"href\")})\n",
    "                if link_data:\n",
    "                    link = link_data[0].get(elem_sel)\n",
    "                    if link:\n",
    "                        next_url = urljoin(current_url, link[0])\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to find next page link with selectors '{block_sel}' and '{elem_sel}': {e}\")\n",
    "        \n",
    "        if not next_url or next_url == current_url:\n",
    "            print(\"No next page link found or link is the same as current page. Stopping.\")\n",
    "            break\n",
    "        \n",
    "        current_url = next_url\n",
    "        counter += 1\n",
    "\n",
    "    return pages\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_multiple_blocks(soup, extraction_jobs):\n",
    "    \"\"\"\n",
    "    Extracts multiple sets of structured data blocks from a single soup object.\n",
    "\n",
    "    :param soup: BeautifulSoup object\n",
    "    :param extraction_jobs: A dictionary mapping data type names to a dict\n",
    "                            containing 'block_selector' and 'fields'.\n",
    "    :return: A dictionary of lists, where each key is a data type (e.g., 'books', 'categories')\n",
    "             and the value is a list of extracted data dictionaries.\n",
    "    \"\"\"\n",
    "    if not soup:\n",
    "        logger.error(\"extract_multiple_blocks() called with no soup object.\")\n",
    "        return {}\n",
    "\n",
    "    all_extracted_data = {}\n",
    "    for job_name, job_config in extraction_jobs.items():\n",
    "        block_selector = job_config.get(\"block_selector\")\n",
    "        fields = job_config.get(\"fields\")\n",
    "\n",
    "        # Reuse the existing extract_blocks function for each job\n",
    "        extracted_data = extract_blocks(soup, block_selector, fields)\n",
    "        all_extracted_data[job_name] = extracted_data\n",
    "    \n",
    "    return all_extracted_data\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def page_processor(soup, extraction_jobs):\n",
    "  return extract_multiple_blocks(soup, extraction_jobs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "_logger = None\n",
    "def get_logger(name=\"books_scraper\"):\n",
    "    \"\"\"Singleton logger to be used across the project.\n",
    "    \n",
    "    This version explicitly deletes the logs directory and its contents\n",
    "    before recreating it to ensure a clean start for each program run.\n",
    "    \"\"\"\n",
    "    global _logger\n",
    "    if _logger:\n",
    "        return _logger\n",
    "\n",
    "    if os.path.exists(\"logs\"):\n",
    "        shutil.rmtree(\"logs\")\n",
    "    \n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "    \n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    if not logger.handlers:\n",
    "        formatter = logging.Formatter(\n",
    "            \"[%(levelname)s] %(filename)s:%(lineno)d %(funcName)s() %(message)s %(asctime)s\"\n",
    "        )\n",
    "\n",
    "        file_handler = logging.FileHandler(\"logs/scraping.log\", mode=\"w\")\n",
    "        file_handler.setFormatter(formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "        console = logging.StreamHandler()\n",
    "        console.setFormatter(formatter)\n",
    "        logger.addHandler(console)\n",
    "\n",
    "    _logger = logger\n",
    "    return logger\n",
    "\n",
    "logger = get_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca26afb4-1c94-433b-b0c4-9b77ca016465",
   "metadata": {},
   "source": [
    "--- extraction fields ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadd1ed8-4166-43b8-b77c-e3261c0d9dfe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "allinone_extractions = {\n",
    "    \"laptops\": {\n",
    "        \"block_selector\": \".product-wrapper.card-body\",\n",
    "        \"fields\": {\n",
    "            \"img_alt\": (\"img\", \"alt\"),\n",
    "            \"img_src\": (\"img\", \"src\"),\n",
    "            \"price\": (\".caption span\", \"text\"),\n",
    "            \"currency\": (\".caption meta\", \"content\"),\n",
    "            \"title\": (\".caption h4 a\", \"title\"),\n",
    "            \"description\": (\".caption p.description\", \"text\"),\n",
    "            \"review-count\": (\".ratings p.review-count span\", \"text\"),\n",
    "            \"rating\": (\".ratings p\", \"data-rating\"),\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_book_extractions = {\n",
    "    \"books\": {\n",
    "        \"block_selector\": \".product_pod\",\n",
    "        \"fields\": {\n",
    "            \"img_href\": (\".image_container a\", \"href\"),\n",
    "            \"img_alt\": (\".image_container a img\", \"alt\"),\n",
    "            \"img_src\": (\".image_container a img\", \"src\"),\n",
    "            \"rating\": (\".star-rating\", \"class\"),\n",
    "            \"book_href\": (\"h3 a\", \"href\"),\n",
    "            \"book_text\": (\"h3 a\", \"text\"),\n",
    "            \"price\": (\".price_color\", \"text\"),\n",
    "            \"availability\": (\".availability\", \"text\"),\n",
    "        }\n",
    "    },\n",
    "    \"categories\": {\n",
    "        \"block_selector\": \".nav-list ul\",\n",
    "        \"fields\": {\n",
    "            \"category_href\": (\"a\", \"href\"),\n",
    "            \"category_text\": (\"a\", \"text\"),\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "allowed_map = {\n",
    "    \"one\": 1,\n",
    "    \"two\": 2,\n",
    "    \"three\": 3,\n",
    "    \"four\": 4,\n",
    "    \"five\": 5\n",
    "}\n",
    "\n",
    "# lambda (selector, attr): (_.get(attr) if (_:= book.select_one(selector)) else \"\") # to avoid NoneType has no attr x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3572212-3245-440b-9419-654ec9430c33",
   "metadata": {},
   "source": [
    "--- books to scrape - site pagination ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f437c2b1-a7d5-4d5b-bce9-043a07d87fb0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://books.toscrape.com\"\n",
    "\n",
    "next_page_selectors = [\n",
    "    (\".pager .next\", \"a\")\n",
    "]\n",
    "\n",
    "scraped_data = pager(\n",
    "    base=url,\n",
    "    max_pages=3,\n",
    "    next_page_selectors=next_page_selectors,\n",
    "    fetcher=make_soup,\n",
    "    callback=page_processor,\n",
    "    extraction_jobs=all_book_extractions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c55030b-b9c7-4b58-afa4-8f6dddf16fe5",
   "metadata": {},
   "source": [
    "--- categories scraped data - data framing ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa13f51-75be-4510-b21e-6afac07ee9f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "categories_data = scraped_data['https://books.toscrape.com']['categories']\n",
    "categories_df = pd.DataFrame(categories_data[0])\n",
    "categories_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91dcd3c-6004-466a-9418-caf1c7a2f905",
   "metadata": {},
   "source": [
    "--- join all scraped pages book records ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bf8798-7f76-43e2-ad29-1965e15465ea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "all_data = []\n",
    "\n",
    "# Loop through all the pages in your scraped data\n",
    "for page_url in scraped_data:\n",
    "    # Check if the page has a 'books' key and if it's not empty\n",
    "    if 'books' in scraped_data[page_url] and scraped_data[page_url]['books']:\n",
    "        # Extend the master list with the books from the current page\n",
    "        all_data.extend(scraped_data[page_url]['books'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d665fbe9-95a4-4f86-88a7-4f044260f533",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "transformer = lambda data: pd.DataFrame([\n",
    "    {key: val[0] if isinstance(val, list) and val else None for key, val in record.items()}\n",
    "    for record in data\n",
    "]) if data else None # get 0th index of all list type values make it new key value\n",
    "\n",
    "# Create the final DataFrame from the combined list of all books\n",
    "df = transformer(all_data)\n",
    "\n",
    "df[\"rating\"] = df[\"rating\"].apply(\n",
    "    lambda x: [allowed_map[stars.lower()] for stars in x if stars.lower() in allowed_map]\n",
    ")\n",
    "df[\"rating\"] = df[\"rating\"].apply(lambda x: x[0] if x else None)\n",
    "\n",
    "\n",
    "df[\"currency\"] = df[\"price\"].str.extract(r\"(^\\D)\")\n",
    "df[\"price\"] = df[\"price\"].str.replace(r\"[^\\d\\.]\", \"\", regex=True).astype(float)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f37d1db-4bff-49ed-8b89-c4cb18de79d0",
   "metadata": {},
   "source": [
    "--- aggregation ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0755d9a0-989d-4362-97c4-9670cd451e90",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "--- all in one - pagination ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4179ac1e-c533-4a25-8eb8-7e60575a981e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://webscraper.io/test-sites/e-commerce/static/computers/laptops\" \n",
    "\n",
    "next_page_selectors = [\n",
    "    (\".page-item\", \"a.next\")\n",
    "]\n",
    "\n",
    "scraped_data = pager(\n",
    "    base=url,\n",
    "    max_pages=3,\n",
    "    next_page_selectors=next_page_selectors,\n",
    "    fetcher=make_soup,\n",
    "    callback=page_processor,\n",
    "    extraction_jobs=allinone_extractions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0d39e4-5023-432c-afce-d65ebba27d11",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "all_data = []\n",
    "\n",
    "for page_url in scraped_data:\n",
    "    if 'laptops' in scraped_data[page_url] and scraped_data[page_url]['laptops']:\n",
    "        all_data.extend(scraped_data[page_url]['laptops'])\n",
    "\n",
    "df = transformer(all_data)\n",
    "df[\"price\"] = df[\"price\"].str.replace(r\"[^\\d\\.]\", \"\", regex=True).astype(float)\n",
    "df[\"price\"] = df[\"price\"].replace(r\"[^\\d\\.]\", \"\", regex=True).astype(float)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
